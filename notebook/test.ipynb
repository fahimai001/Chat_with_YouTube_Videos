{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c574d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you're a Python developer and you're building with LLMs and you want to learn how to integrate MCP, the model context protocol, then this video is for you. I'm going to walk you through my entire crash course that I put together to really learn what we can do with MCP as Genai developers. And now in this video, I'm not just going to show you how you can build a simple server and integrate it into Claw Desktop, for example. I've seen a lot of tutorials covering just that. This is really developer first tutorial. So, you're going to learn how you can set up your own MCP servers using the Python SDK, but then also integrate those servers into your own Python application and back end so you can start to build AI systems and agents around it. And now, if you're new to the channel, my name is Dave Ablar. I'm the founder of Data Luminina. We've been building custom data and AI solutions for our clients for the past six years already. So, even before Chat GBT, I have a bachelor's and a master's degree in artificial intelligence. And my goal with these videos is always to really help you to cut through the noise and focus on what you can actually use to build production ready AI systems and agents. So let's dive in. All right. So throughout this tutorial, I will be walking you through this GitHub repository. Link will be in the description. It's the MCP crash course which is part of my AI cookbook where I dump all of the tutorials for all of the videos that I create here on this channel. So let's quickly cover what we will go through in this video. And we're going to start with some theory first. So I will outline uh timestamps below this video. So if you want to dive straight into the code, you feel free to jump to that, but I don't recommend doing that if you're new to MC MCP because you really need to first understand what it is because otherwise a lot of things won't make any sense. So I will walk you through seven chap chapters. Chapter one, chapter two is really about understanding what MCP is and then we're going to set up the servers. We do an open AAI integration. We'll look at MCP versus function calling. Uh we'll show you how you can set up your server and run it in Docker so you can actually start to put this into applications and then finally we'll talk about life cycle management to properly handle all the connections of your applications. Now if you want to follow along feel free to clone this repository, set up your dev environment, install the requirements should be straightforward and then all of the underlying code examples that I will walk you through are in the folders over here. All right, and with that out of the way let's dive straight into part one introduction and context. So, for those of you that are completely new to MCP, it stands for model context protocol. And over the past weeks and months, it has been gaining a lot of popularity, but it's already been out since November 25th, 2024. It was developed by Entropic. And in this blog post, they introduced MCP as a new standard for connecting AI assistance to systems where data lives, including content repository, business tools, and development environments. And I remember back in November when I first read this blog post that I found it pretty hard to wrap my head around what it exactly was. Is this a new model? Is it are these some new capabilities? Is this part of cloud? Can we integrate with OpenAI? All of these questions that I had as a developer to see how can we literally translate this to the client project that we're currently working on. And since there are major new things and updates in the world of AI about every other week, I'm always very hesitant to dive into new things like this. So I really want to avoid chasing all the shiny objects and change our project structure and architecture about every month and MCP was one of those shiny objects for me. So I ignored it for a while. But then this happened. This is Google Trends where you can see the interest over time for the keyword MCP on Google. And you can see that from beginning of March 2025, this popped off exponentially and I started to see MCP everywhere. I saw it on YouTube. I saw it on X on blog posts and in my community data freelancer. It started to appear everywhere and I could no longer ignore this. So what happened in February that suddenly caused this major increase in interest for MCP and to explain that we first need to understand high level what MCP is. So if you do a quick Google search you'll find many diagrams trying to explain really what this is about. So I found this one on dcope.com and I think this is a very good starting point for us to all get on the same page and remember the definition from entropic. So MCP is a new standard for connecting AI assistance to systems where data lives and this image over here illustrates that perfectly. So on the left over here we have the situation before MCP. So this is what we as developers have been doing for two and a half years, three years already. So we built applications around LLMs and then when we want to connect our app to an external surface like Slack, Google Drive or GitHub, we create our own API layer where for Slack for example, we have a function that is send message and with that we can communicate with Slack. And now when we want to integrate that with our LLM, we either create custom logic and at some point in our application when we have all the data, we send it or when we want the LLM to dynamically use that tool, we can do that through fun function calling and for example using a tool definition and then providing that to the LLM. So this is what we've been doing for quite some time already. And this is exactly the key point why I was first very hesitant about diving into MCP because fundamentally as we will discover MCP doesn't introduce any new functionality to a large language model or to building AI systems that we couldn't do in the past years already. It's rather just a new way of doing things. And that's exactly what you see over here on the right where now in the middle we have this layer over here, this protocol that's going to unify an API that we make available to an LLM. And now the benefits of this is that now rather than each and every developer on the world creating their own kind of definitions for how to integrate with Slack with Google Drive and all kinds of systems, we now have a standard protocol that exactly defines how you should specify your schemas and your functions and even your your documentation and arguments. So when you give it to the MCP layer, your AI applications can now seamlessly integrate with that via a unified API. So now that we understand that MCP fundamentally doesn't add any new capabilities to working with LLMs, but rather is just a standardized way of making tools and resources available to LLMs. We can start to see what happened in February. So MCP is technically a very solid and lightweight protocol and more and more companies and developers started to realize this where over time if more and more people would use MCP we could standardize and make our life as developers a lot easier by not every time reinventing the wheel when we for example wanted to connect our AI services to Slack or GitHub or Google Drive etc. And this trend started to pick up where now there are already hundreds of officially supported surfers by major tech companies and all kinds of tools and apps that expose their API via an MCP server which now makes it very easy for you as a developer to come in and start integrating your AI systems and agents with these tools that you see over here for example. And this hype continued to now even open AAI entropic's biggest competitor supporting MCP in their agent SDK which resulted in MCP essentially winning which is outlined in his excellent blog post here by Leighton Space and despite all the critique around the hype on social media right now. So here's an example just saw someone use MCP to drive a car to the grocery store remotely to buy a sausage bread. LLM drove it with MCP on airplane Wi-Fi. pretty much explaining that everyone is just using MCP for MCP to solve all kinds of problems around LLMs where you really don't need MCP. But despite all that, the number one thing that MCP has going on right now is the adaptation rate where the power of any new protocol derives from its adaptation aka the ecosystem. And if we look at the GitHub star history from some of the other popular AI frameworks and libraries that simplify building apps with LLMs, we can clearly see the exponential trend that MCP is on right now, which is nothing compared to anything we've seen so far. And it will most likely overtake all of them in the next few months. All right, so now we understand from a high level what MCP does, and we even covered a little bit of history that future engineers might even learn similar to how they learned about HTTP. But anyway, now you might be wondering, \"Okay, Dave, I kind of like get it, but what as developers can we do with it?\" So, let's cover that right now. All right, so let's get into part two, understanding MCP at a technical level. And if you're like, \"Dude, stop talking. Just show me the goat.\" Again, feel free to skip to the next section. This is the last theoretical part. I'll cover this briefly, but again, there are some fundamental things that you need to understand about MCP in order to work with it effectively. So you need to understand some common terminology that we will cover and that I will explain using this image over here which I copied from the official MCP documentation. You will find these terms throughout the uh documentation as well throughout as well as throughout the video tutorials here that I have. So let's start with hosts. So hosts are programs like claw desktop idees any other application or more interesting for developers your Python application uh but that wants to access data through the MCP protocol and then you have your MCP clients which are your protocol clients that maintain a one-on-one connections with your servers which brings us to the next part MCP servers. So MCP servers are lightweight programs that each expose specific capabilities through the standardized model context protocols. And what they can expose are tools, resources, and prompts. More on that later. So these three components really make up the core architecture of MCP. But through MCP, you can also connect with local data sources as well as remote services via the internet, for example, via APIs. And hey, if you're liking this video so far, please leave a like and also consider subscribing and make sure to share it with your team so they can also learn about how to integrate MCP effectively in the projects you're working on. And now let's look at the following diagram to better understand that. So all the way on the left over here, we have the hosts with the MCP client. So again, this can be anything like claw desktop IDE tools or your custom Python back end or application. Then in the middle layer over here, we have servers. So here you can see we have server A, B and C which can all have a different purpose. They have different tools and definitions, different utilities that we can use them for. Now your host through the MCP client over the MCP protocol can connect to that server and then interact with that. So that is how the communication is set up and within your host application you can essentially pull data from the MCP server. And now there are a lot of tutorials online that show you how you can set up a server and then integrate it with cloud desktop. All the initial tutorials that I found online were always about cloud desktop. And I couldn't really fully understand this from a developer perspective where right now what you should really consider is that there is a really clear distinction between using MCP for local tools that you use as personal AI assistance so that you as a user use in your application in cloud desktop or cursor or windsurf or you as a developer creating a surfer and then creating a client application on your own that connects with that server. That's really a key distinction that we will cover further in this video. And now then on your MPC server, you can define your application specific logic. And this can be anything. But for this context and to better understand it as a developer, you can literally just think about this as any Python function. You can put any kind of function. Whatever the outcome, whether it's a simple calculation, putting a string in uppercase or connecting to your database, it doesn't matter. you can create a function and that is the utility that is the logic that you put on that server that you can then make available to your host application. So when it comes to working with MCP as a developer you need to understand three things. The first one is understanding how you can set up a server. Now this can be a server you build on your own with custom logic which is what we're going to do in this video or you can simply copy and paste some of the example servers that are already available for some of the companies and apps listed here or that you'll find online. So that's part one, setting up a server. Then the second thing that you need to understand is how you can set up a host application and then connect with your server through the client. So this is also new and this is something that we're also going to cover of course within this video. And our client in this case is going to be our own custom Python back end. And then the final part that we need to understand is this right layer over here and that's simply how to connect local data sources or remote services to a server via Python. And this is nothing new. This is just writing simple Python functions to implement the logic that you need. All right. And there's one more thing that you need to understand about MCP and it's probably the most important thing to understand about this whole protocol as a developer. And until I fully understand what this was, I still had a hard time to wrap my head around the utility of MCP. And that concept is about the two transport mechanisms that you can use with MCP, namely standard IO and server send events. So to understand that let's first get back to this image over here where we can see everything happening on the same machine except for the remote services but the MCP server and the MCP host with the CL MCP client all lives on the same computer. So this could be your laptop but this could also be the server that you deploy it to. And when I first started to explore MCP, I really had this question with like what's the point of setting up a dedicated server and then connecting that to your application if everything lives on the same machine. To me, this just felt like a very cumbersome way of defining your tools for your LLM and defining your AI application and then having to spin up a server and then make the connection rather than just having a simple Python file, call it tools.py, Pi and import that. So I really couldn't wrap my head around like why would you make things so complex if you just run it on your own computer and yeah okay I could understand how you have all of these predefined servers available that you could quickly copy paste into your project so they could help you to save a little bit of time on development right but at least for the projects we were working on most of the tools that we were using were limited to either one two three and these were just simple Python function that you could probably spin up with uh with cloth or cursor in like one prompt So, I still couldn't really see the full utility of this until I learned about the different ways that you can really set this up because if you follow the quick starts and if you follow most of the tutorials that you'll find online, everyone is using the standard IO method where everything just lives on your local machine. You spin it up and from one single Python file, you essentially connect to the server and you run it. And there's just this added layer of complexity that you need to implement. But you can also take your server and put it really somewhere else. So this could be on a remote server on a completely different machine. And then you can connect to it via server events using HTTP. And now your MCP server is accessible via an API. And the different mechanisms are visualized over here where if we focus on remote development, I highlighted with the different colors that this is a different machine, different server that you're interacting from. So you as a developer could have one server where you maintain all your tools and then you have could have various client applications that connect to that same server. So now you can share multiple resources, multiple tools throughout the different projects and applications that you're building. And you could also then in parallel spin up multiple servers. So you can have multiple servers, multiple clients. And this is really a fundamental component to understand as a developer because if you're using the standard IO transport, it's all uh restricted to local development. And I I still believe right now that is just a very like cumbersome way of making tools available for an LLM. And quick note on that one, I'm referring to using MCP in your own custom Python back ends. So not in using for example cloud desktop or other application because for that using the standard IO on your local machine is still very effective because you can simply create the servers and these tools on your local machine can use them very effectively without having to do complex integrations. But this is really where I see most of the utility in. So that's also really what we're going to cover. And I haven't seen a lot of tutorials really get into this and showing you examples of about how to actually put this into an application, set up the server via Docker, connect to it, and build real applications. So that's a very critical distinction to understand. So now that you know all the theory, let's dive into some practical examples where I'm also going to clearly show you the differences between standard IO and using HTTP. And now the nice thing for us as Python developers is that there is an official Python SDK for MCP. And this is going to make our lives a lot easier because as you'll find out with this SDK, we can simply create surfers and connect to them in really just a few lines of Python. All you really need is to add the following pip package MCP CLI to your environment and literally you're good to go. So if you want to learn more about the underlying SDK, for example, some of the parameters that you can tweak and really want to go deep, this is the resource to go through. All right, so let's set up our first server. All right, so I am in the repository over here and we're going to be working from part three right now. Simple server setup. At the top level of this crash course, there is a requirements.txt. So, make sure you have a virtual environment set up. Install this. I uh nowadays do this via UV. If you don't know what UV is, you should really look into that. It's awesome. It's much better than just using pip. So, with that out of the way, make sure that you can run this code. And let's get into this very simple server. As you can see, it's about like 31 lines of Python code. It's very simple. And let's cover these one by one to understand what's going on. Now the purpose of this repository is not only to teach you how you can set up your servers but also to act as really a boilerplate or a starting point when you want to build new servers. So that's what I've been doing. So I have these templates laying around. So I don't have to remember the syntax. I can simply come in here and copy paste it. So to set this up all we really need to do to create an MCP server is do this. It's literally as simple as this. And this is possible because of the Python SDK that we have installed. And what you can immediately see is that it's very similar to what you would see if you work with for example fast API. And this is also how I want you to really start thinking about MCP. So with fast API to create a Python back end to create an API also what you have to do is you import fast API then you create your app. This is literally the same. they probably looked at fast API and copied some of these uh the design principles from there because I don't think this is a coincidence. So all we really need to do is right now is we can give it a name but this is all optional. We can leave it empty and for demonstration I've already put in a host and a port here. Now, without diving into the details just yet, if you want to use uh your server via HTTP, so you want to have it on a separate machine, you need to uh specify uh the host and the port. But if you do the standard IO, which you'll see in most tutorials that you'll find online, this is not even needed because we simply connect with it via our our local file system and just point to the path. But that's the first step in setting up our server. We call fast MPC and we have this instant over here. Now the second thing that you most likely want to do is you want to add tools to your server and they made this really simple as well. You can simply add tools using this decorator. So we can call the at MCP where MCP is referring to this name. So if this has another name let's call let's call this as then this should also be ass. So that's just a reference that's in there. And then we call tool and we can use this decorator to essentially take a function and say this is now a tool for our MCP server and we can simply copy paste this and now uh we have at three and we can have another function in here and you get the idea. Now this is about as simple as as it gets for defining a function but this can be anything. This can be connecting to your database. This can be connecting to Slack and sending a message. You can do anything in here as long as it's a Python function. It has arguments to uh put in here and you add the decorator. If you do that, if you follow that syntax, your MCP server will now make that available and list it really as as a resource that can be used. Now then coming to the bottom of this script over here. Here is the part to run this server. And there's a little bit of if else logic in here, but this is simply to make a distinction between running your server in the two different transport mechanisms that we discussed. So SSC or standard IO and most likely in a production environment or a serious application, you want to load this from an environment variable. But simply for the sake of demonstration, I just hardcoded it in here and so I can easily switch within this tutorial when I show the different client implementations in a bit. So I'm actually going to start with the standard IO mechanism where this if statements is going to hit because transport is standard IO. And then here you can see how this is going to be run. So it's MCP.run and we have transport and we put in standard IO. So again MCP refers to the MCP server that we created. So this is about as minimal as it gets when setting up an MCP server. And then here in the docs you can see an even simpler example if you use the standard IO method. So remember if it's all on your machine, one computer, you can simply do this. You just give to give it a name and then you can simply run it. So now that we have the server over here, server.py, what we can do is we can run this in a development mode. And this is a really cool way to inspect your MCP server and to test it. So you can do this with the MCP inspector. So what you can do if you come to your project, I'm here inside cursor. I've opened up a terminal. Make sure you're in the correct folder. So I need to cd into simple server setup. So here is the server that we want to run. And what you can then do is you can do mcp devaf and then server.py and then when we run that it's going to spin up that uh server. And first it's going to ask if you do this for the first time it's going to ask you to download the inspector. So I just hit y for that to download. And once that download is complete it spins up a server that we can now follow. And here you can see we are now inside the MCP inspector. So what we can do is we can hit connect and we can connect to our server. And here we can see everything that we can access. And most of it is empty right now. But there's one thing and that's going to be the tools because we have a very simple uh server with only one tool. So we can come in here and we can say list tools. And you can see we have one tool which is called add. Add add two numbers together. And you can see it simply copies all of the information from from in here. So the function is or the tool is called add based on the function name. And then we have the description which is simply the dock string over here that is added in here. And then we have the parameters A and B. So we can plug this in, we can test it and we get the result back six. Now, I think this is a very nice and neat way to test the various components that make up your server without actually having to uh write code for every component in order to test it. So, this inspector is also really great for debugging and to potentially explore some of the other things that you can implement on your server like prompts and resources where I don't see much use cases for those right now because you could for example also just make data or local data available via a tool. But if you go to the documentation for for example the resources here we can see the Python uh implementation on your server similar to how you can add a tool you can also list resources and read resources and you could specify local file pots in here. So let's say you have a server uh you dump some data on there and you want to make that available to your AI application. This is the way you do it. But typically what we see nowadays is you would either do this through some kind of like ragp pipeline in a factor database. So again I don't see much use cases for this right now but it's good that it's already implemented so that you can access local files through the server as well. And essentially the same thing for prompts. So you can register prompts. So here you can see define available prompts and then we can come to our server and you can create a list to store those prompts here. Again, right now, I could see how long-term if you really like fully integrate within the MCP ecosystem, having prompts in here that you can reuse throughout different projects can be nice as well. But right now, there are so many other ways to do this as well, like just doing it doing it in your project or using tools like Langfuse. Uh again, this is more of of the same what what we already could. And I see really the most added value right now is is really in the tools. But I want this to be a complete crash course. So you can reference to the docs here as well. So this is in part two, the readme file. Uh those are all the links to the official documentation where if you have a use case where you want to store prompts in there or local files, that is the way to go. And then real quick, if you're serious about learning Genai and you want to learn how to build production ready AI systems and agents, but you might be coming from a different role or background or you're new to AI at all, you might want to check out the first link in the description. It's a video of me going over how my company can help you with this. We're releasing our Gen AI accelerator program, which is our training program that we use to train our internal developers to work on the client projects that we're currently working on at data luminina. And we're making it available to the public as well. So, if you really want to do this professionally and want to learn AI engineering from first principles, you might want to check it out. All right, so we just saw how we could create a simple server, then spin it up and connect to it via the MCP inspector. But now let's go one step further and use this client standard io.py file to see how we can connect to this server from within our Python application. And we're going to use the Python SDK for this as well. So that this is really simple for us. So here you can see again just a few lines of code. And let's see what this looks like. So I'm going to run this in an interactive Python session. This is how I always like to uh run my code. it really helps me with debugging and checking all of the data variables. So you essentially use uh a Jupyter session behind the scene and that's why I use this nest_ynio.apply. If you just run this in the in the terminal uh like regular people, you don't need this and that's because out of the box this interactive session via Jupiter can't handle async processes and we have async functions in here. So that's why we have that in here. But let me show you what that looks like. And by the way, if you want to learn more about how I run this code, you can check out the link in the description where I have a full video covering how I set up my development environment. That's where I cover this. So let's cover what we're doing over here. We start with a simple main function and first we define our uh server parameters. So this is very simple. We're going to run this server with Python and the argument that we provide to it is a ser is our server.py file. So a context is always important here. So we're running everything from within this folder over here. So both the server and the client are in here. That's why we can simply refer to server.py which is going to refer to this file and we simply use the command python. So we run python server.py. So it's going to boot up this server behind the scenes. And since we have it to standard IO, this is going to call mcp.run and this will spin up our server. Now for context again we're all doing this on the same computer. So essentially through our host application we're going to call our server uh but we can do it in such a way where we don't have to spin it up first and then call our application. We just put the commands in here and the Python SDK takes care of that. So that's a really convenient way to uh to work with this. So here we can now look at how we connect to that server and this is just syntax from the SDK again where we set up the connection and we essentially create a session and we initialize it and this is going to run the or use the server parameters in order to make the connection. So let's see what that looks like if I store this in memory. So we have our server params in here and then we can set up a session and when I run that nothing will happen just a check that we now have initialized a session. So now let's see what we can do in here. So one of the things that we can do is we can list the available tools and with that we can essentially use our session and we can call the method list tools. Now again this is the same we can do the same thing for resources and prompts but we're going to solely focus on tools right now. So since this is uh really a session and we use the width, we really need to run everything together in order for the context to remain open. So I need to select all of this and I can run this and I can see right now if I go through this I list the tools and I print it. I can see available tools add and here we can see our simple calculator tool that we have that we also saw in the inspector. Right? So this is the only tool that's available. And now in the next step I could see uh our calculator tool. So we can use that and in order to do that we do again we're all using the session throughout this whole process and we do the call tool and we use the at and we put in the following arguments. So that's how we use our calculator tool. So let's now look at what we can do over here. And here you can see we have the tools we print it and now we call our tools as well. So now if I close this out again and I can use this whole process and run it at once again you will see the same operation. So what we have now essentially done we have effectively created the server. We have defined those server parameters on our client application using standard IO. We create a connection to that server. We use that and we call it our session. And then using that session object over here, we can interact with our server using the various methods that are available on here. For example, list tools and call tools. And now you're probably like, \"Okay, Dave, I get it. But this again, this feels still like a very cumbersome way to make a function available in another file within the same project. Why not just import it?\" And you're totally right. There's also no AI or any LLM integration in this example. And that's fundamentally really what MCP comes down to. It's just a standardized way of transferring information in the forms of tools, prompts, and and data from one way to really another place, but then in a way that is easily accessible for large language models. So, in the next section, we'll definitely go a lot deeper and I will show you how you can integrate LLMs with MCP. But for now, we still have to cover the SSE transport mechanism via HTTP. And in order to set that up, I'm going to come back to my server and now set the transport mode to SSE. Set that up. And now I can open up the client over here. So now what we can do is we have another client. It's very similar to what we saw in the standard IO mechanism, but now there is a different way that we connect to our server. And here you can see since we're running on the same local machine, we connect to local host and we use the port that we specified in our setup over here. So this refers to local host and then here's the port. So if I come over here right now within the client standard IO, I could just run this and I didn't need to spin up the server because the Python SDK takes care of that by creating the session using the server per apps. But if I now let's say if I come in here and I try to run this, I will get all kinds of errors because this session over here that we try to create, it cannot connect to the server. And that's because currently there's nothing going on at this address. So that's why it also says here make sure the server is running. And in order to do that, we need to again open up a terminal. We need to be in this folder. And we can call u runser.py or you can also just do python server.py. So depending on how you want to run it. So let's open up a new terminal. So let's see. Let's go into our MCP/crash course and we should be in still in chapter number three. So in here we have our server. So now I can call uv run server.py and this is going to spin up our server. So you can see this is now running on localhost port 8050. And now once this running, if I close this window over here, I can run this whole file and I will have the exact same output over here because it's literally doing the same thing. So it's opening up the session although right now via a different transport mechanism via SSE and it's creating the session and after that it's exactly the same. It's just a different way of establishing the session. And again I want to highlight this over here. So this really comes back to are you working with local development where the standard IO is possible but you can also do it via this method that I just explained over here where you're essentially using an API and using an HTTP request. But if you were to put this on a different server or a different machine, this is the only way that you can do it. All right, so by now you already understand the most fundamental aspects that you need to know about working with MCP. And this knowledge alone will probably already put you in the top 5% of developers and people working with MCP. But now let's go one step deeper and see how you can actually start to build applications around LLMs using MCP. And for that we're going to spin up a new server. Now we're inside part number four over here. So this is the OpenAI integration. I'll scroll down uh through the server file. And you can see it's very simple. Again, we set up the server. We use all the same settings and we have one tool. And now this is a very simplified tool and this is just as a demonstration to emulate rack. So retrieval augmented generation. So what we're doing is I created a data folder in here and in there is a JSON file. So there are five question answer pairs from a hypothetical company knowledge base. So for example, what's our company uh vacation policy? Now typically you within an AI application you would put this into a factor database and then through rack you can retrieve the most relevant chunks or results and then provide that to the LLM in order to give your application domain specific knowledge. Now we're going to emulate that in in this very simple example here. So this is a tool that is called get knowledge base. So the goal of this tool is to retrieve the entire knowledge base as a formatted string. So we're not going to do filtering or selection. We're just going to take the whole thing and provide it to the LLM. But you could simply copy this example and instead of what I do right here, implement your logic to perform similarity search or hybrid search and retrieve the top top K top five results, for example. It's all the same. It's just about a different way of implementing it. And that's really what you should keep in mind when working with these tools. I'm showing you simple examples, but this can be any function you like. And then we at the end we simply have the run statement where we run the server and we use the standard IO example in this case because it's all on my local machine and it's just a little bit simpler to set that up. All right. So nothing new on the server side. But now let's look at the client. And there's a little bit more code to this because right now the client is not only about creating the connection and then listing the tools and just like doing a simple calculation. Even though this is really about, okay, how would we build a client that can integrate with or connect to an MCP server, get tools from it, and then also actually use those and integrate them with AI. So, I created this MCP OpenAI client, and it's a class, and I will be walking you through the various functions that we have in here in order to show you what this can do. So, first, let me scroll down to give you a a full look of everything that's going on here. And then let's come all the way down to the bottom. So what we can do with this file is we can essentially establish the client by creating uh a new instance really of this class that we created and then we can connect to the server through the function that we have implemented over here. So you can see right now the connection part on the client. I've simply put that into a server. You can see again this is all similar stuff. You have the server parameters. We run it via Python. We just link the server script because again we're working locally, same machine, standard IO. You now know how the stuff works. The server is in the same folder. That's why why we call it server.py. And now here's how we can connect to it and we can create that session and list the tools. So that's the first step in let's see starting to work with it. So now we can create this client. We can connect to this server. So when we have that we can now start to ask questions and that is because this client that we created also has a process query function in here and here you can see where we can start to integrate the AI components. So when we call this process query we can put in a query which is a string. So which is the question. So now you can start to imagine okay as a developer you're building an AI system and some information is coming in from a user. Let's say you're building a chatbot and now you want to take that answer and you want it through run a run it through like the rack pipeline get the information and then synthesize the results. That's what's going on over here. But rather than hard- coding everything, we're going to make this tool that we created on our server, which is the get knowledge base. We're going to make that tool available to our large language model. And we're going to do that through MCP. And if you're already familiar with working with tools here, you can already start to compare the different options, right? Again, this is nothing new. This is also something that we could just do in this file. So in this client.py, I could have simply created that tool, then make it available in here, but we load it from our server because of all of the benefits around MCP that we described. But again, it's always important to compare what would this look like if I weren't using MCP and what does it look like right now. You fully need to understand that as a developer in order to make the right design choices and architecture choices in your applications because you don't need it every time. So the first step in this process query is getting the MCP tools and that's another function that we implemented in here which is something we already saw right we have our session and we can list the tools. So this is something we did in the previous example. And now all I'm doing in this example since we're working with OpenAI, I am going to take that tool and I'm going to look at the data schema and I'm going to put it in the exact syntax that the OpenAI API requires. So this is the the function definition really of what a tool looks like and this is something specified by OpenAI. So we adhere to this so that that we know when we put our tools in this structure, we can send it to OpenAI and OpenAI will know how to work with it. So that's important to keep in mind here. That's simply what we're doing. So let me make this a little bit more practical by showing you what we can do here. So again I'm spinning up a session and here you can can again see what's convenient about this way of running the code. So we can here essentially let's see we can create the server and it will run this and it says connected to the server and then it lists the tools. So we have the get knowledge base. So we now know that our MCP server has access to this tool. Now we can then come here and we have this query. So let's ask what is our company uh vacation policy. So this is an an a question that can be answered using this element here in the knowledge base. And now what's going to happen in the next step if we come back to our client you can see that we're going to um create this response and then print it. So this is the final answer that we would be getting back from our system. So let's quickly see what's going on over here. So we get the tools then we put it into we get it into the right format that OpenAI expects and then we make an LLM uh or we make an API call to OpenAI. So if you've worked with OpenAI, this is nothing new for you. We use the chat completions uh API. We put we plug in a model which we specified earlier on in the initialization of this class. So we use GPT40 in this case. Scrolling down. Then we create a message object where we simply we have no system prompt. We just say user content and we put in the question. So what's the company's vacation policy? And now here's where it's get interesting. So now what we do is since we now have the tools which are available on our MCP server and we have converted them into the right format that OpenAI expects using get MCP tools we can now add that to our call and for to understand this and what's going on behind the scenes you not you don't necessarily need to understand anything about MCP but you need to understand how tools and function calling works with large language models. Now, if you're completely new to this, I have other videos covering this as well, but essentially all it does is the LLM considers your whole request. So, your your question, what you what you send your prompt, what you send to the API, and also the available tools. And because of the description that you have within your tools, it can decide to call that tool yes or no. And calling a tool simply means looking at the the the schema or the function and the parameters that you need within that function and then looking at the context of the user question and then providing you with the parameters that you need to plug into the function that you can run in your application. And now I fully understand that is pretty hard and abstract to wrap your head around the first time if you're new to this. So let me show you the example end to end and then I'm going to break this down step by step to really see all of the different building blocks that come together in here. So this is within the uh MCP OpenAI client again. And since this is all in a class, we can just run this in a nice and neat way all at once. So what's our company's uh vacation policy? I'm going to run this. It's going to list the tools. It's going to run the query and then in the background the LLM is going to use the tool retrieve the right information via the MCP layer and then provide that back to OpenAI in order to give us the final reply. Our company's vacation policy is as follows which it got from our knowledge base which is essentially uh accessed through our server even though the files are right next to each other but this is accessed through the server. So now like I've said let's come back to this simple implementation over here. And what I did is that I essentially took this nice and neat client that is all wrapped up into a nice class that you would use in a production environment for example I have now broken it down into separate functions. And the nice thing about this is if we're using the interactive uh Python window that I'm using on the right all the time we can walk through this one by one one step at a time. So let me show you what that looks like and we can completely follow the order of operations in here. So again we have our server. So this is already we know how to do this. We connect with our server again same machine standard IO. We have our question what's our company vacation policy. Now here if I were to uh run this function over here it will go through the whole thing. But what I can now also do is if I come over here and I come into this process query over here, I know the query is stored in memory. So I can now take the global session OpenAI client and model and I can get the available tools. So I can now look in here and I can now see in action what this function is doing. So before I just showed you the code, right? But now we can also look at the output. So here you can see the schema or the tools really that are defined that we can now send to open AAI. So now we're here at the chat completions part again and we're going to run this the first time where we say we we provide it with our tools and we leave the tool choice to auto. So what we can now do is we run this we send it to OpenAI and let's have a look at the message. And now if I come in here and if I scroll to the right, I can see in here that there is a tool call. So OpenAI decided to use the tool rather than provide us directly with an answer to the question. And now what you need to do in order to build applications around this and again this is not MC MCP specific. This is specific to how you integrate tools within your applications. You essentially construct the message again but now also adding the intermediate assistant message to this which specifies that hey first we have the question from the user but now we did a tool call and we have the following information available. So we have a name which is get knowledge base and then we also have the arguments which in this case are non-existent because there are no arguments in this uh in this uh function over here. But typically it would also give for example a key or a question that you would use for rack. But now coming back what we can then do is we do another loop. We do another iteration where we essentially loop over the message from the assistant. We check if there's a tool call in it. And if that is the case we call our tool. So here we can see this will be true if I if I run this because this exists because we have a tool call and now we can say for the tool call in the assistant uh message we can get the we can get the result. So let's see what it looks like. And we can get the result in here. And now we can see that we have some actual data in here. And that is because we have our session. Remember this is our MCP session that we can use to call our tools. And we get the right tool by getting the tool called function.name. And this is what we got back from uh OpenAI. And then we can also load the arguments in there which are currently empty. But that is the this is really a fundamental thing that you need to understand and I know it can seem very abstract right now if you're going through this for the first time. But this example will really teach you the order of operations and flow of data. So OpenAI decides to use the tool then provides the parameters. We need to parse that in our application then call the actual function which lives on the server. then retrieve those results, append it to our message objects again. So that now you can see we have the complete context for our AI model. So you can see what's our company policyification policy. This is the original question. Now we have the assistant that decided to uh call the tool which is over here and also the result of that tool which is the content is the retrieved knowledge base. So here you can see this is the whole JSON file that is now within context and if you've been building with LLMs you now know that if you have this message object and you send this to OpenAI it will figure out how to solve that because it has all the components and you can now also in this second operation you uh you could even even leave those out or you could have tool uh choice none. So to really be be clear about that that right now it has all of the information and we can run that and we do a second API call to OpenAI to get the final result which is if I get that over here you can see that it now has the final answer that it would send back. So now coming back to our proper implementation of the client that is everything that's going on behind the scenes over here. So when I run this code, it's going to connect to the server, have the query, send it to OpenAI. This is everything that's going on behind the scenes. And now if you want to have more tools available for your AI system, you simply need to implement them in your server and then also check the logic over here to make sure you that you can catch those tool calls. So what happens if the LLM decides to call that tool and what and how do you need to append that to the context or what do you need to do with it and how do you need to call it? All right, so I hope you're still with me. That was definitely the hardest part about this whole crash course and to fully understand this. You likely need to experiment with the examples, adjust it, create your own examples and run those to really get the hang of it. But now finally, I want to cover some practical tips and examples that you also need to understand that will help you to build with MCP. All right, let's get into part five. MCP versus function calling. And this is a very short one. Throughout this video, you've heard me mention this already a couple of times. MCP adds nothing new. This is stuff that we have been able to do for 2 years already. And this short file over here just highlights that where I just outline a simple tool call where we specify a function within the same file. So here you can see we have an OpenAI chat completion. We handle the tool calls and we send it one more time to OpenAI. And when we run all of this, we have the exact same setup where really the only distinction is instead of getting our tools from the MCP server, we have it directly in our file. And now this example should also help you to answer the question, should I migrate all of my existing AI projects to MCP? And the answer is most likely no. So if you're already using function calling or or using tools and it's working perfectly right now, adding MCP to that is not going to change anything. It's just going to make things a little bit more complex in the beginning. But as you by now should understand going forward long-term, it might make sense to for new projects that rely heavily on using tools, it might make sense to look at some MCP standard surface or create your own tools and integrate it that way. But there's no immediate need or necessity to really migrate your current projects. All right. Now, I hope you're still with me. I know this is a long one, but we need to cover two more concepts in order to make this a true complete crash course for developers. And next, I want to cover running MCP servers with Docker because in my opinion, in in the real world, this is probably going to be the most practical way that you're going to use MCP servers that don't live on your local machine, but that you actually deploy somewhere and that you can reuse for other applications. That's really where you start to get the benefits of M MCP rather than just having extra complexity within your projects. So I created a simple setup to show you how to do this. So coming back to the code over here, there's going to be nothing new. We have a simple server. So this is the exact same server that we already covered. We also have a very simple client that we can run. And again, this is all the same. The only thing that's new over here is we have a Docker file. And that docker file essentially wraps this server.py file into a nice docker container which we can then run and then within that environment it's going to install the requirements and it's going to run the server so that it then becomes available. So the instructions to do that are outlined over here. So you can follow the instructions and the first thing that we need to do is we need to build this. So we can do this by coming into this folder and opening up a terminal in here. Now what I can then do is I can come in here and I can paste the docker build tmcp server command. So what this is going to do it's going to build the docker container using docker. So make sure you have uh the docker demon running on your system. And when you run that it's going to build the docker images which is then going to be available via docker on your local machine. And what you can then do is let me clear this up for you. You can run the following command and that is docker run. And then we just refer to the image that we created the MCP server and make sure we run it on the correct port. So let's start this up. And now we can see that this is running over here. So remember this is very similar to what we were doing previously when we were setting up our server and running it via either via uvun server.py or python server.py and running it and then connecting to it via SSE. So this is exactly the same thing but now it's just in a docker container and we can now use our client in exactly the same way because it's exposed on localhost under this port and we can run this again and perfectly it connects to our server and it runs the calculations and now once this is packaged in a docker container what you can do with this is you can create a virtual machine or you could use a manage resource on HNER AWS or Azure doesn't really matter and you could clone the repository on the system there. Run the startup commands to boot the Docker image and then connect to your server remotely and now not via local host but you will do it for the domain or IP address that your server is on or the managed server is on and then you now have all of your server and your tools living somewhere else and now from any other application within the world or machine you can connect to it. All right. And now you made it to the end of this crash course, the final part, part seven, life cycle management. And I briefly want to cover this and more so just point you to this resource that you can later reference. And when you're building real applications with this and you're putting them into production and you are for example connecting with various data sources or other external surfaces, it's important to properly manage your your life cycles and all the connections that you have especially with things like databases. And in this file, I've first of all outlined the life cycle management upon initialization, operations, and then the termination. So this is all already handled in the way I've set up the code examples because we're using the width operator really in Python. So that automatically handles some of the sessions. But for a more advanced life cycle management approach, you can create a lifespan object. And here's an example from the official MCP documentation. And let me scroll down over here. So yeah, so when you create an MCP server, you can also add a live spawn to this. And this can be useful when you are for example connecting to databases. So you really make sure that when we close the connection the connection to that database is also like gracefully shut down and handled properly by properly in implementing something to disconnect from your database rather than just shutting it down. And I wanted to put this out here just again to really be a complete guide, a complete crash course to working with MCP because when you're working with surfers and clients and you're going to work with many different components going on, having a proper life cycle of everything that's going on is really important long term as your servers grow and as maybe also the clients working with those surfers grow. So in the beginning you don't really have to worry much about this but this is really for example when you're working in a company an enterprise and your teams are building multiple servers multiple clients you're connecting with databases this is definitely something you want to look into. All right and that brings us to the end of this crash course. So I hope that by now you have a very solid understanding of what MCP is and how you can use it as a Python developer to not only integrate it into claw desktop or cursor but really use it in your own backends to build AI systems and agents. And now don't forget to check out the Genai Accelerator first link in the description if you really want to learn AI engineering from first principles. And now, if you're already a skilled AI engineer and you've been building with LLMs for some time, but you just need a better way to get your apps to production, you might want to check out our Genai launchpad, which is our production framework that we use for all of our projects inside data luminina. So, as part of this, you'll get access to this repository. And we use this as our boilerplate to start every client project that we do. And this literally saves us about two to three weeks of setup time on every project. It's a complete AI backend and infrastructure that you can use to quickly spin up your projects. All right, and then that's it for this video. So, thank you for your time and attention going through this entire crash course. If you found it helpful, please leave a like and also consider subscribing if you aren't already. And then next up, I recommend checking out this video where I go over some patterns that you can use to build effective AI agents, but doing it in pure Python without needing any frameworks.\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import re\n",
    "\n",
    "def get_transcript(url, lang='en'):\n",
    "    video_id = re.search(r\"(?:v=|youtu\\.be/)([a-zA-Z0-9_-]{11})\", url).group(1)\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[lang])\n",
    "        return ' '.join(entry['text'] for entry in transcript)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=5xqFjh56AwM&t=2268s\"\n",
    "print(get_transcript(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc82fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chat_with_YouTube_Videos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
